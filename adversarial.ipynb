{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SE8ALUNO\\Anaconda3\\envs\\myenv\\lib\\site-packages\\numpy\\lib\\arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt\n",
    "# import warnings\n",
    "# import sys\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from genericpath import isfile\n",
    "from TrafficLabelling.utils import *\n",
    "\n",
    "\n",
    "dataset = 'lycos-ids2017/' # TrafficLabelling lycos-ids2017\n",
    "modo = 'pca_9' # cos pca_9\n",
    "segunda = 'knn' # knn mlp\n",
    "f = 'CPd_dest' # in ['CPdId','CPeIe','CPd_dest','C']:\n",
    "if dataset == 'lycos-ids2017/':\n",
    "    dataset_clean = pd.read_csv(dataset + 'dataset_clean_exp_atrib.csv', index_col=[0])\n",
    "    dataset_clean = pd.get_dummies(dataset_clean, columns=['ip_prot'],drop_first=True)\n",
    "\n",
    "    # Comentar caso use Pd = 6\n",
    "    dataset_clean_cat = pd.read_csv(dataset + 'pd4.csv', index_col=[0])\n",
    "    dataset_clean = pd.concat([dataset_clean_cat, dataset_clean.loc[:,np.invert(dataset_clean.columns.str.contains('[sd]port'))]], axis=1)\n",
    "    del dataset_clean_cat\n",
    "\n",
    "    Id = dataset_clean.columns.str.contains('[sd]addr')\n",
    "    Id = dataset_clean.columns[Id].to_list()\n",
    "    Pd = dataset_clean.columns.str.contains('[sd]port')\n",
    "    Pd = dataset_clean.columns[Pd].to_list()\n",
    "\n",
    "    Ie = dataset_clean.columns.str.match('(src_addr)|(dst_addr)')\n",
    "    Ie = dataset_clean.columns[Ie].to_list()\n",
    "    Pe = dataset_clean.columns.str.match('(src_port)|(dst_port)')\n",
    "    Pe = dataset_clean.columns[Pe].to_list()\n",
    "\n",
    "    C = np.setdiff1d(dataset_clean.columns,Id+Pd+Ie+Pe+['label','flow_id','timestamp']).tolist()\n",
    "    Cs = ['flow_duration', 'down_up_ratio', 'pkt_len_max', 'bytes_per_s', 'pkt_per_s', 'fwd_pkt_cnt', 'fwd_pkt_len_tot', 'fwd_pkt_len_max'] + dataset_clean.columns[dataset_clean.columns.str.contains('prot')].to_list()\n",
    "\n",
    "    # comb_f = {'CPe':C+Pe,'CPd'+str(int(len(Pd)/2)):C+Pd,'PeIe':Pe+Ie,'Pd'+str(int(len(Pd)/2))+'Ie':Pd+Ie,'Pd'+str(int(len(Pd)/2))+'Id'+str(int(len(Id)/2)):Pd+Id,'CPd'+str(int(len(Pd)/2))+'Ie':C+Pd+Ie,'CPd'+str(int(len(Pd)/2))+'Id'+str(int(len(Id)/2)):C+Pd+Id,'C':C,'CPeIe':C+Pe+Ie,'CPd'+str(int(len(Pd)/2))+'_dest':C+Pd[-int(len(Pd)/2):],'CPe_dest':C+Pe[-6:],'Cs':Cs,'CsPd'+str(int(len(Pd)/2))+'_dest':Cs+Pd[-int(len(Pd)/2):]}\n",
    "    comb_f = {'CPdId':C+Pd+Id,'C':C,'CPeIe':C+Pe+Ie,'CPd_dest':C+Pd[-int(len(Pd)/2):],'Cs':Cs,'CsPd'+str(int(len(Pd)/2))+'_dest':Cs+Pd[-int(len(Pd)/2):]} # para manter compatibilidade com o TrafficLabelling\n",
    "    # f = 'CPd'+str(int(len(Pd)/2))+'Id'+str(int(len(Id)/2))\n",
    "\n",
    "    rest_cols = comb_f[f]\n",
    "\n",
    "elif dataset == 'TrafficLabelling/':\n",
    "    # dataset original\n",
    "\n",
    "    C = ['Protocol', 'FlowDuration', 'TotalFwdPackets', 'TotalBackwardPackets', 'TotalLengthofFwdPackets', 'TotalLengthofBwdPackets', 'FwdPacketLengthMax', 'FwdPacketLengthMin', 'FwdPacketLengthMean', 'FwdPacketLengthStd', 'BwdPacketLengthMax', 'BwdPacketLengthMin', 'BwdPacketLengthMean', 'BwdPacketLengthStd', 'FlowBytes/s', 'FlowPackets/s', 'FlowIATMean', 'FlowIATStd', 'FlowIATMax', 'FlowIATMin', 'FwdIATTotal', 'FwdIATMean', 'FwdIATStd', 'FwdIATMax', 'FwdIATMin', 'BwdIATTotal', 'BwdIATMean', 'BwdIATStd', 'BwdIATMax', 'BwdIATMin', 'FwdPSHFlags', 'FwdURGFlags', 'FwdHeaderLength', 'BwdHeaderLength', 'FwdPackets/s', 'BwdPackets/s', 'MinPacketLength', 'MaxPacketLength', 'PacketLengthMean', 'PacketLengthStd', 'PacketLengthVariance', 'FINFlagCount', 'SYNFlagCount', 'RSTFlagCount', 'PSHFlagCount', 'ACKFlagCount', 'URGFlagCount', 'CWEFlagCount', 'ECEFlagCount', 'Down/UpRatio', 'AveragePacketSize', 'AvgFwdSegmentSize', 'AvgBwdSegmentSize', 'SubflowFwdPackets', 'SubflowFwdBytes', 'SubflowBwdPackets', 'SubflowBwdBytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward', 'ActiveMean', 'ActiveStd', 'ActiveMax', 'ActiveMin', 'IdleMean', 'IdleStd', 'IdleMax', 'IdleMin']\n",
    "\n",
    "    Cs = ['Protocol', 'ACKFlagCount', 'ActiveMean', 'ActiveMin', 'AveragePacketSize', 'BwdIATMean', 'BwdPacketLengthMin', 'BwdPacketLengthStd',\n",
    "                 'BwdPackets/s', 'FwdIATMean', 'FwdIATMin', 'FwdPacketLengthMean', 'FwdPackets/s', 'FwdPSHFlags', 'FlowDuration', 'FlowIATMean',\n",
    "                 'FlowIATMin', 'FlowIATStd', 'Init_Win_bytes_backward', 'Init_Win_bytes_forward', 'PSHFlagCount', 'SubflowFwdBytes', 'SYNFlagCount',\n",
    "                 'TotalLengthofFwdPackets']\n",
    "\n",
    "    Pd = ['sPort0', 'sPort1', 'sPort2', 'sPort3', 'sPort4', 'sPort5', 'dPort0',\n",
    "           'dPort1', 'dPort2', 'dPort3', 'dPort4', 'dPort5']\n",
    "\n",
    "    Id = ['sIP0', 'sIP1', 'sIP2', 'sIP3', 'dIP0', 'dIP1', 'dIP2', 'dIP3']\n",
    "\n",
    "    # Ie = dataset_clean.columns.str.match('(SourceIP)|(DestinationIP)')\n",
    "    # Ie = dataset_clean.columns[Ie].to_list()\n",
    "    Ie = ['SourceIP_DNS_int', 'SourceIP_Reverse_proxy', 'SourceIP_Web_Server_int', 'SourceIP_ext', 'DestinationIP_DNS_int', 'DestinationIP_Reverse_proxy', 'DestinationIP_Web_Server_int', 'DestinationIP_b_m_cast', 'DestinationIP_ext']\n",
    "    # Pe = dataset_clean.columns.str.match('(SourcePort)|(DestinationPort)')\n",
    "    # Pe = dataset_clean.columns[Pe].to_list()\n",
    "    Pe = ['SourcePort_444', 'SourcePort_80', 'SourcePort_8080', 'SourcePort_dynamic', 'SourcePort_registered', 'SourcePort_system', 'DestinationPort_444', 'DestinationPort_80', 'DestinationPort_8080', 'DestinationPort_dynamic', 'DestinationPort_registered', 'DestinationPort_system']\n",
    "    comb_f = {'CsPe':Cs+Pe,'CPe':C+Pe,'CsPd':Cs+Pd,'CPd':C+Pd,'PeIe':Pe+Ie,'PdIe':Pd+Ie,'PdId':Pd+Id,'CsPdIe':Cs+Pd+Ie,'CsPdId':Cs+Pd+Id,'CPdIe':C+Pd+Ie,'CPdId':C+Pd+Id,'C':C,'Cs':Cs,'CPeIe':C+Pe+Ie,'CPd_dest':C+Pd[-6:]}\n",
    "    # f = sys.argv[2]\n",
    "    rest_cols = comb_f[f]\n",
    "\n",
    "    # dataset_clean = pd.read_csv(path + 'dataset_clean_exp_atrib.csv', index_col=[0])\n",
    "    dataset_clean = pd.read_csv(dataset + 'dataset_clean_exp_atrib.csv', usecols=rest_cols+['label'])\n",
    "    # dataset_clean = load_chunk('dataset_clean_exp_atrib.csv',s=20000)\n",
    "    # dataset_clean = dataset_clean[rest_cols+['Label']]\n",
    "    dataset_clean.rename(columns={'Label':'label'}, inplace=True)\n",
    "\n",
    "labels = dataset_clean['label']\n",
    "features = dataset_clean.loc[:, rest_cols].astype('float64')\n",
    "del dataset_clean\n",
    "LE = LabelEncoder()\n",
    "LE.fit(labels)\n",
    "labels = LE.transform(labels)\n",
    "\n",
    "features_train_val, features_test, labels_train_val, labels_test = train_test_split(features, labels, test_size=.2, random_state=42, stratify=labels)\n",
    "\n",
    "features_train, features_val, labels_train, labels_val = train_test_split(features_train_val, labels_train_val, test_size=.2, random_state=42, stratify=labels_train_val)\n",
    "\n",
    "m_train = features_train.shape[0]\n",
    "m_val = features_val.shape[0]\n",
    "m_test = features_test.shape[0]\n",
    "m = labels.shape[0]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train)\n",
    "\n",
    "features_train = pd.DataFrame(scaler.transform(features_train), index=features_train.index,columns=features_train.columns)\n",
    "# features_train_val = scaler.transform(features_train_val)\n",
    "features_val = pd.DataFrame(scaler.transform(features_val), index=features_val.index,columns=features_val.columns)\n",
    "features_test = pd.DataFrame(scaler.transform(features_test), index=features_test.index,columns=features_test.columns)\n",
    "\n",
    "del features_train_val\n",
    "del labels_train_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similaridade: 0.9900385173421111\n",
      "||VP|| = 0.012315348333560067\n",
      "||FP|| = 1.0910840641851431\n",
      "Usando somente a confiança:    611 FP em 7773 exemplos (7.86%)\n",
      "Usando somente a confiança:    150 FP em 1940 exemplos (7.73%)\n",
      "Usando somente a confiança:    216 FP em 2447 exemplos (8.83%)\n"
     ]
    }
   ],
   "source": [
    "features = {'train':features_train,'val':features_val, 'test':features_test}\n",
    "labels = {'train':labels_train, 'val':labels_val, 'test':labels_test}\n",
    "model_path = dataset+'saved_models/bin_'+f\n",
    "model = tf.keras.models.load_model(model_path+'.h5',compile=False)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.trainable = False\n",
    "\n",
    "# _, train_acc = model.evaluate(features_train, labels_train>0, verbose=0)\n",
    "# _, val_acc = model.evaluate(features_val, labels_val>0, verbose=0)\n",
    "# _, test_acc = model.evaluate(features_test, labels_test>0, verbose=0)\n",
    "# # print('Acurácias (%):\\nTeste: {:2.4f}'.format(100*test_acc))\n",
    "# print('Acurácias (%):\\nTreino: {:2.4f}\\nValidação: {:2.4f}\\nTeste: {:2.4f}'.format(100*train_acc,100*val_acc,100*test_acc))\n",
    "\n",
    "def create_adversarial_pattern(input_sample):#, input_labels):\n",
    "    loss_object = tf.keras.losses.binary_crossentropy\n",
    "    input_label = tf.constant(1,dtype='float32',shape=(input_sample.shape[0],1))\n",
    "    input_sample = tf.convert_to_tensor(input_sample)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_sample)\n",
    "        prediction = model(input_sample)\n",
    "        loss = loss_object(input_label, prediction)\n",
    "\n",
    "    # Get the gradients of the loss w.r.t to the input sample.\n",
    "    gradient = tape.gradient(loss, input_sample)\n",
    "    # gradient = tape.gradient(prediction, input_sample)\n",
    "    # Get the sign of the gradients to create the perturbation\n",
    "    # signed_grad = tf.sign(gradient)\n",
    "    prediction = tf.squeeze(prediction).numpy()\n",
    "\n",
    "    return prediction, gradient.numpy()#, msc_p, msc_fp\n",
    "\n",
    "\n",
    "for cj in ['train','val','test']:\n",
    "\n",
    "    prediction, gradient = create_adversarial_pattern(features[cj].values)#, labels[cj])\n",
    "    msc_p = prediction > 0.5\n",
    "    msc_fp = msc_p & (labels[cj] == 0)\n",
    "    if cj == 'train':\n",
    "        if dataset+f == 'TrafficLabelling/CPdId':\n",
    "            conf_limiar = prediction[msc_fp].max()\n",
    "        else:\n",
    "            conf_limiar = np.percentile(prediction[msc_fp],99, interpolation='higher')\n",
    "\n",
    "        msc_vp = msc_p & (labels[cj] != 0)\n",
    "\n",
    "        VP = gradient[msc_vp].mean(axis=0)# CPdId, CPdIe\n",
    "        FP = gradient[msc_fp].mean(axis=0)# CPdId, CPdIe\n",
    "        # VP = gradient[msc_vp & (prediction <= conf_limiar)].mean(axis=0)\n",
    "        # FP = gradient[msc_fp & (prediction <= conf_limiar)].mean(axis=0)\n",
    "        vp_fp = np.array([VP, FP])\n",
    "\n",
    "        # Similaridade entre a média dos gradientes (VP e FP) no conjunto treino\n",
    "        print('Similaridade:',float(cosine_similarity(VP[np.newaxis,:], FP[np.newaxis,:])))\n",
    "        # Normas das médias dos gradientes (VP e FP) no conjunto treino\n",
    "        norma_vp_fp = np.linalg.norm(vp_fp, axis=1)\n",
    "        print('||VP|| = {}\\n||FP|| = {}'.format(norma_vp_fp[0],norma_vp_fp[1]))\n",
    "        dl = {}\n",
    "\n",
    "    if modo == 'cos':\n",
    "        dados = np.concatenate((prediction[msc_p,np.newaxis],cosine_similarity(gradient[msc_p], vp_fp), np.linalg.norm(gradient[msc_p],axis=1, keepdims=True),labels[cj][msc_p,np.newaxis] == 0), axis=1)\n",
    "        dados = pd.DataFrame(dados, index=features[cj][msc_p].index, columns=['Confiança','cos(\\u03B8)\\u2207_VP','cos(\\u03B8)\\u2207_FP','||\\u2207||','FP?'])\n",
    "\n",
    "    elif modo == 'pca':\n",
    "        dados = np.concatenate((prediction[msc_p,np.newaxis],gradient[msc_p], labels[cj][msc_p,np.newaxis] == 0), axis=1)\n",
    "        dados = pd.DataFrame(dados, index=features[cj][msc_p].index, columns=['Confiança']+features[cj].columns.tolist()+['FP?'])\n",
    "\n",
    "    elif modo == 'pca_9':\n",
    "        if cj == 'train':\n",
    "            from sklearn.decomposition import PCA\n",
    "            n=9\n",
    "            pca = PCA(n_components=n)\n",
    "            pca.fit(gradient)\n",
    "\n",
    "        gradient = pca.transform(gradient)\n",
    "        dados = np.concatenate((prediction[msc_p,np.newaxis],gradient[msc_p], labels[cj][msc_p,np.newaxis] == 0), axis=1)\n",
    "        dados = pd.DataFrame(dados, index=features[cj][msc_p].index, columns=['Confiança']+['pca'+str(i) for i in range(n)]+['FP?'])\n",
    "\n",
    "    dados.sort_values('Confiança',ascending=False,inplace=True)\n",
    "    dados = dados.astype({'FP?': 'bool'})\n",
    "\n",
    "    dl[cj] = dados[dados['Confiança']<=conf_limiar]\n",
    "\n",
    "    refin = 0\n",
    "    cf_fp, cf_size = dl[cj]['FP?'].sum(), dl[cj].shape[0]\n",
    "    # adv_fp, adv_size = (((dl[cj]['cos(\\u03B8)\\u2207_FP']-dl[cj]['cos(\\u03B8)\\u2207_VP'])>=refin) & dl[cj]['FP?']).sum(), ((dl[cj]['cos(\\u03B8)\\u2207_FP']-dl[cj]['cos(\\u03B8)\\u2207_VP'])>=refin).sum()\n",
    "    # cf_idx = dl[cj][dl[cj]['FP?']].iloc[-adv_fp].name\n",
    "    # cf_idx_size = cf_size - np.flatnonzero(dl[cj].index == cf_idx).squeeze()\n",
    "    \n",
    "    print('Usando somente a confiança: {:>6d} FP em {:>4d} exemplos ({:2.2f}%)'.format(cf_fp, cf_size, 100*cf_fp/cf_size))\n",
    "    # print('Usando confiança + Aprox. Adv.: {:>2d} FP em {:>4d} exemplos ({:2.2f}%)'.format(adv_fp, adv_size, 100*adv_fp/adv_size))\n",
    "    # print('A partir do menos confiável: {:>5d} FP em {:>4d} exemplos ({:2.2f}%)\\n'.format(adv_fp, cf_idx_size, 100*adv_fp/cf_idx_size))\n",
    "    \n",
    "\n",
    "labels['train'] = dl['train']['FP?']\n",
    "features['train'] = dl['train'].iloc[:, :-1]\n",
    "# LE.fit(labels['train'])\n",
    "# labels['train'] = LE.transform(labels['train'])\n",
    "scaler.fit(features['train'])\n",
    "features['train'] = scaler.transform(features['train'])\n",
    "labels['val'] = dl['val']['FP?']\n",
    "features['val'] = dl['val'].iloc[:, 0:-1]\n",
    "# labels['val'] = LE.transform(labels['val'])\n",
    "features['val'] = scaler.transform(features['val'])\n",
    "\n",
    "labels['test'] = dl['test']['FP?']\n",
    "features['test'] = dl['test'].iloc[:, 0:-1]\n",
    "# labels['test'] = LE.transform(labels['test'])\n",
    "features['test'] = scaler.transform(features['test'])\n",
    "\n",
    "cj='test'\n",
    "if (dataset == 'TrafficLabelling/') & (f in ['C','CPd_dest']):\n",
    "    p = np.linspace(0,.25,11)\n",
    "else:\n",
    "    p = np.linspace(0,1,11)\n",
    "fracoes = dl[cj].shape[0]*p\n",
    "fracoes = fracoes.round().astype(int)[1:]\n",
    "# print(p)\n",
    "# print(fracoes)\n",
    "\n",
    "tot_fp = msc_fp.sum()\n",
    "dx = p[2]-p[1]\n",
    "cj_analise = np.zeros(fracoes.shape,dtype='int')\n",
    "for i,j in enumerate(fracoes):\n",
    "    cj_analise[i] = dl[cj].iloc[-j:,-1].sum()\n",
    "# print(cj_analise)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FP total no dl: 216 de 219\n",
      "\n",
      "\n",
      "\n",
      "AUC: 0.817 (sem 2ª rede)\n",
      "\n",
      "\n",
      "[[ 63.           0.28767123]\n",
      " [116.           0.52968037]\n",
      " [157.           0.71689498]\n",
      " [180.           0.82191781]\n",
      " [197.           0.89954338]\n",
      " [198.           0.90410959]\n",
      " [199.           0.9086758 ]\n",
      " [210.           0.95890411]\n",
      " [214.           0.97716895]\n",
      " [216.           0.98630137]]\n",
      "\n",
      "AUC: 0.817 (atb = 3, neighbors=19)\n",
      "\n",
      "\n",
      "AUC: 0.567 (val)\n",
      "\n",
      "\n",
      "[[ 69.           0.31506849]\n",
      " [116.           0.52968037]\n",
      " [170.           0.77625571]\n",
      " [191.           0.87214612]\n",
      " [204.           0.93150685]\n",
      " [205.           0.93607306]\n",
      " [206.           0.94063927]\n",
      " [212.           0.96803653]\n",
      " [213.           0.97260274]\n",
      " [216.           0.98630137]]\n",
      "\n",
      "AUC: 0.842 (atb = 6, neighbors=19)\n",
      "\n",
      "\n",
      "AUC: 0.579 (val)\n",
      "\n",
      "\n",
      "[[ 66.           0.30136986]\n",
      " [120.           0.54794521]\n",
      " [162.           0.73972603]\n",
      " [188.           0.85844749]\n",
      " [202.           0.92237443]\n",
      " [203.           0.92694064]\n",
      " [203.           0.92694064]\n",
      " [212.           0.96803653]\n",
      " [213.           0.97260274]\n",
      " [216.           0.98630137]]\n",
      "\n",
      "AUC: 0.834 (atb = 10, neighbors=19)\n",
      "\n",
      "\n",
      "AUC: 0.573 (val)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nFP total no dl: {:d} de {:d}\\n\\n'.format(cf_fp,tot_fp))\n",
    "# print(np.column_stack((cj_analise, cj_analise/tot_fp)))\n",
    "print('\\nAUC: {:.3f} (sem 2ª rede)\\n\\n'.format(np.trapz(cj_analise/tot_fp, dx=dx)/((len(cj_analise)-1)*dx)))\n",
    "\n",
    "\n",
    "rodadas = 20\n",
    "\n",
    "if modo == 'pca_9':\n",
    "    atbs = np.floor(np.linspace(0,dados.shape[1]-1,4)[1:]).astype(int)\n",
    "else:\n",
    "    atbs = [1, dados.shape[1]-1]# colocar 1 antes\n",
    "\n",
    "for atb in atbs:\n",
    "    cj_analise = np.zeros(fracoes.shape)\n",
    "\n",
    "    # atb = dados.shape[1]-1 # 1 para apenas confinça, 4 para confiança + aprox adv\n",
    "    # atb = 1 # 1 para apenas confinça, 4 para confiança + aprox adv\n",
    "\n",
    "    if segunda == 'mlp':\n",
    "        dl2 = np.zeros(labels[cj].shape)\n",
    "        for n in range(rodadas):\n",
    "            model2 = tf.keras.models.Sequential([tf.keras.layers.Dense(10, input_dim=features['train'][:,:atb].shape[1], activation='relu'), tf.keras.layers.Dense(10, activation='relu'), tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "            model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', verbose=1, patience=5, restore_best_weights=True)\n",
    "            model2.fit(features['train'][:,:atb], labels['train'], epochs=150, verbose=0, batch_size = 256, validation_data=(features['val'][:,:atb], labels['val']), callbacks=[es])\n",
    "\n",
    "            dl2 += model2.predict(features[cj][:,:atb]).squeeze()\n",
    "\n",
    "        dl2 = dl2/rodadas\n",
    "        msg = ''\n",
    "\n",
    "    elif segunda == 'knn':\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        fracoes_val = dl['val'].shape[0]*p\n",
    "        fracoes_val = fracoes_val.round().astype(int)[1:]\n",
    "        # cj_analise = np.zeros(fracoes.shape)\n",
    "        aucs = np.zeros(rodadas)\n",
    "        for l in range(1, rodadas):\n",
    "            model2 = KNeighborsClassifier(n_neighbors=l)\n",
    "            model2.fit(features['train'][:,:atb], labels['train'])\n",
    "            dl2 = model2.predict_proba(features['val'][:,:atb])\n",
    "            dl2 = pd.DataFrame(np.column_stack((dl2[:,1],labels['val'])),index=labels['val'].index,columns=['Conf2',labels['val'].name])\n",
    "            dl2.sort_values(by=['Conf2'], inplace=True)\n",
    "            for i,j in enumerate(fracoes_val):\n",
    "                cj_analise[i] = dl2.iloc[-j:,-1].sum()\n",
    "            aucs[l] = np.trapz(cj_analise/tot_fp, dx=dx)/((len(cj_analise)-1)*dx)\n",
    "\n",
    "        n_neighbors=aucs.argmax()\n",
    "        model2 = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "        model2.fit(features['train'][:,:atb], labels['train'])\n",
    "        dl2 = model2.predict_proba(features['test'][:,:atb])[:,1]\n",
    "        msg = ', neighbors={:d}'.format(n_neighbors)\n",
    "\n",
    "    dl2 = pd.DataFrame(np.column_stack((dl2,labels[cj])),index=labels[cj].index,columns=['Conf2',labels[cj].name])\n",
    "    dl2.sort_values(by=['Conf2'], inplace=True)\n",
    "    for i,j in enumerate(fracoes):\n",
    "        cj_analise[i] = dl2.iloc[-j:,-1].sum()\n",
    "    \n",
    "    # print('\\natb: {:d}\\nFP total no dl: {:d} de {:d}\\n'.format(atb,cf_fp,tot_fp))\n",
    "    print(np.column_stack((cj_analise, cj_analise/tot_fp)))\n",
    "    print('\\nAUC: {:.3f} (atb = {:d}{:s})\\n'.format(np.trapz(cj_analise/tot_fp, dx=dx)/((len(cj_analise)-1)*dx), atb, msg))\n",
    "    print('\\nAUC: {:.3f} (val)\\n\\n'.format(aucs.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "atb: 4\n",
      "FP total no dl: 309 de 318\n",
      "\n",
      "AUC: 0.818\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dl2_path = 'C:/Users/SE8ALUNO/maj_ricardo/google/doutorado/duvidas/'+dataset+'adv/dl2_'+f+'.csv'\n",
    "\n",
    "# dl2.rename(columns={'Conf2':'Conf2_adv','FP?':'FP?_adv'}, inplace=True)\n",
    "# dl2.to_csv(dl2_path)\n",
    "dl2 = pd.read_csv(dl2_path,index_col=0,float_precision='round_trip')\n",
    "\n",
    "for i,j in enumerate(fracoes):\n",
    "  cj_analise[i] = dl2.iloc[-j:,-1].sum()\n",
    "print('\\natb: {:d}\\nFP total no dl: {:d} de {:d}\\n'.format(atb,cf_fp,tot_fp))\n",
    "print('AUC: {:.3f}\\n'.format(np.trapz(cj_analise/tot_fp, dx=dx)/((len(cj_analise)-1)*dx)))\n",
    "# print(np.column_stack((cj_analise, cj_analise/tot_fp)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "375.994px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "485.188px",
    "left": "645.125px",
    "right": "20px",
    "top": "174.938px",
    "width": "628.75px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
