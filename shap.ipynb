{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import os\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "# import sys\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from genericpath import isfile\n",
    "from TrafficLabelling.utils import *\n",
    "\n",
    "\n",
    "dataset = 'TrafficLabelling/' # TrafficLabelling lycos-ids2017\n",
    "modo = 'pca'\n",
    "segunda = 'knn' # mlp\n",
    "f = 'CPdId' #['CPdId','CPeIe','CPd_dest','C']:\n",
    "if dataset == 'lycos-ids2017/':\n",
    "    dataset_clean = pd.read_csv(dataset + 'dataset_clean_exp_atrib.csv', index_col=[0])\n",
    "    dataset_clean = pd.get_dummies(dataset_clean, columns=['ip_prot'],drop_first=True)\n",
    "\n",
    "    # Comentar caso use Pd = 6\n",
    "    dataset_clean_cat = pd.read_csv(dataset + 'pd4.csv', index_col=[0])\n",
    "    dataset_clean = pd.concat([dataset_clean_cat, dataset_clean.loc[:,np.invert(dataset_clean.columns.str.contains('[sd]port'))]], axis=1)\n",
    "    del dataset_clean_cat\n",
    "\n",
    "    Id = dataset_clean.columns.str.contains('[sd]addr')\n",
    "    Id = dataset_clean.columns[Id].to_list()\n",
    "    Pd = dataset_clean.columns.str.contains('[sd]port')\n",
    "    Pd = dataset_clean.columns[Pd].to_list()\n",
    "\n",
    "    Ie = dataset_clean.columns.str.match('(src_addr)|(dst_addr)')\n",
    "    Ie = dataset_clean.columns[Ie].to_list()\n",
    "    Pe = dataset_clean.columns.str.match('(src_port)|(dst_port)')\n",
    "    Pe = dataset_clean.columns[Pe].to_list()\n",
    "\n",
    "    C = np.setdiff1d(dataset_clean.columns,Id+Pd+Ie+Pe+['label','flow_id','timestamp']).tolist()\n",
    "    Cs = ['flow_duration', 'down_up_ratio', 'pkt_len_max', 'bytes_per_s', 'pkt_per_s', 'fwd_pkt_cnt', 'fwd_pkt_len_tot', 'fwd_pkt_len_max'] + dataset_clean.columns[dataset_clean.columns.str.contains('prot')].to_list()\n",
    "\n",
    "    # comb_f = {'CPe':C+Pe,'CPd'+str(int(len(Pd)/2)):C+Pd,'PeIe':Pe+Ie,'Pd'+str(int(len(Pd)/2))+'Ie':Pd+Ie,'Pd'+str(int(len(Pd)/2))+'Id'+str(int(len(Id)/2)):Pd+Id,'CPd'+str(int(len(Pd)/2))+'Ie':C+Pd+Ie,'CPd'+str(int(len(Pd)/2))+'Id'+str(int(len(Id)/2)):C+Pd+Id,'C':C,'CPeIe':C+Pe+Ie,'CPd'+str(int(len(Pd)/2))+'_dest':C+Pd[-int(len(Pd)/2):],'CPe_dest':C+Pe[-6:],'Cs':Cs,'CsPd'+str(int(len(Pd)/2))+'_dest':Cs+Pd[-int(len(Pd)/2):]}\n",
    "    comb_f = {'CPdId':C+Pd+Id,'C':C,'CPeIe':C+Pe+Ie,'CPd_dest':C+Pd[-int(len(Pd)/2):],'Cs':Cs,'CsPd'+str(int(len(Pd)/2))+'_dest':Cs+Pd[-int(len(Pd)/2):]} # para manter compatibilidade com o TrafficLabelling\n",
    "    # f = 'CPd'+str(int(len(Pd)/2))+'Id'+str(int(len(Id)/2))\n",
    "\n",
    "    rest_cols = comb_f[f]\n",
    "\n",
    "elif dataset == 'TrafficLabelling/':\n",
    "    # dataset original\n",
    "\n",
    "    C = ['Protocol', 'FlowDuration', 'TotalFwdPackets', 'TotalBackwardPackets', 'TotalLengthofFwdPackets', 'TotalLengthofBwdPackets', 'FwdPacketLengthMax', 'FwdPacketLengthMin', 'FwdPacketLengthMean', 'FwdPacketLengthStd', 'BwdPacketLengthMax', 'BwdPacketLengthMin', 'BwdPacketLengthMean', 'BwdPacketLengthStd', 'FlowBytes/s', 'FlowPackets/s', 'FlowIATMean', 'FlowIATStd', 'FlowIATMax', 'FlowIATMin', 'FwdIATTotal', 'FwdIATMean', 'FwdIATStd', 'FwdIATMax', 'FwdIATMin', 'BwdIATTotal', 'BwdIATMean', 'BwdIATStd', 'BwdIATMax', 'BwdIATMin', 'FwdPSHFlags', 'FwdURGFlags', 'FwdHeaderLength', 'BwdHeaderLength', 'FwdPackets/s', 'BwdPackets/s', 'MinPacketLength', 'MaxPacketLength', 'PacketLengthMean', 'PacketLengthStd', 'PacketLengthVariance', 'FINFlagCount', 'SYNFlagCount', 'RSTFlagCount', 'PSHFlagCount', 'ACKFlagCount', 'URGFlagCount', 'CWEFlagCount', 'ECEFlagCount', 'Down/UpRatio', 'AveragePacketSize', 'AvgFwdSegmentSize', 'AvgBwdSegmentSize', 'SubflowFwdPackets', 'SubflowFwdBytes', 'SubflowBwdPackets', 'SubflowBwdBytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward', 'ActiveMean', 'ActiveStd', 'ActiveMax', 'ActiveMin', 'IdleMean', 'IdleStd', 'IdleMax', 'IdleMin']\n",
    "\n",
    "    Cs = ['Protocol', 'ACKFlagCount', 'ActiveMean', 'ActiveMin', 'AveragePacketSize', 'BwdIATMean', 'BwdPacketLengthMin', 'BwdPacketLengthStd',\n",
    "                 'BwdPackets/s', 'FwdIATMean', 'FwdIATMin', 'FwdPacketLengthMean', 'FwdPackets/s', 'FwdPSHFlags', 'FlowDuration', 'FlowIATMean',\n",
    "                 'FlowIATMin', 'FlowIATStd', 'Init_Win_bytes_backward', 'Init_Win_bytes_forward', 'PSHFlagCount', 'SubflowFwdBytes', 'SYNFlagCount',\n",
    "                 'TotalLengthofFwdPackets']\n",
    "\n",
    "    Pd = ['sPort0', 'sPort1', 'sPort2', 'sPort3', 'sPort4', 'sPort5', 'dPort0',\n",
    "           'dPort1', 'dPort2', 'dPort3', 'dPort4', 'dPort5']\n",
    "\n",
    "    Id = ['sIP0', 'sIP1', 'sIP2', 'sIP3', 'dIP0', 'dIP1', 'dIP2', 'dIP3']\n",
    "\n",
    "    # Ie = dataset_clean.columns.str.match('(SourceIP)|(DestinationIP)')\n",
    "    # Ie = dataset_clean.columns[Ie].to_list()\n",
    "    Ie = ['SourceIP_DNS_int', 'SourceIP_Reverse_proxy', 'SourceIP_Web_Server_int', 'SourceIP_ext', 'DestinationIP_DNS_int', 'DestinationIP_Reverse_proxy', 'DestinationIP_Web_Server_int', 'DestinationIP_b_m_cast', 'DestinationIP_ext']\n",
    "    # Pe = dataset_clean.columns.str.match('(SourcePort)|(DestinationPort)')\n",
    "    # Pe = dataset_clean.columns[Pe].to_list()\n",
    "    Pe = ['SourcePort_444', 'SourcePort_80', 'SourcePort_8080', 'SourcePort_dynamic', 'SourcePort_registered', 'SourcePort_system', 'DestinationPort_444', 'DestinationPort_80', 'DestinationPort_8080', 'DestinationPort_dynamic', 'DestinationPort_registered', 'DestinationPort_system']\n",
    "    comb_f = {'CsPe':Cs+Pe,'CPe':C+Pe,'CsPd':Cs+Pd,'CPd':C+Pd,'PeIe':Pe+Ie,'PdIe':Pd+Ie,'PdId':Pd+Id,'CsPdIe':Cs+Pd+Ie,'CsPdId':Cs+Pd+Id,'CPdIe':C+Pd+Ie,'CPdId':C+Pd+Id,'C':C,'Cs':Cs,'CPeIe':C+Pe+Ie,'CPd_dest':C+Pd[-6:]}\n",
    "    # f = sys.argv[2]\n",
    "    rest_cols = comb_f[f]\n",
    "\n",
    "    # dataset_clean = pd.read_csv(path + 'dataset_clean_exp_atrib.csv', index_col=[0])\n",
    "    dataset_clean = pd.read_csv(dataset + 'dataset_clean_exp_atrib.csv', usecols=rest_cols+['label'])\n",
    "    # dataset_clean = load_chunk('dataset_clean_exp_atrib.csv',s=20000)\n",
    "    # dataset_clean = dataset_clean[rest_cols+['Label']]\n",
    "    dataset_clean.rename(columns={'Label':'label'}, inplace=True)\n",
    "\n",
    "labels = dataset_clean['label']\n",
    "features = dataset_clean.loc[:, rest_cols].astype('float64')\n",
    "del dataset_clean\n",
    "LE = LabelEncoder()\n",
    "LE.fit(labels)\n",
    "labels = LE.transform(labels)\n",
    "\n",
    "features_train_val, features_test, labels_train_val, labels_test = train_test_split(features, labels, test_size=.2, random_state=42, stratify=labels)\n",
    "\n",
    "features_train, features_val, labels_train, labels_val = train_test_split(features_train_val, labels_train_val, test_size=.2, random_state=42, stratify=labels_train_val)\n",
    "\n",
    "m_train = features_train.shape[0]\n",
    "m_val = features_val.shape[0]\n",
    "m_test = features_test.shape[0]\n",
    "m = labels.shape[0]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train)\n",
    "\n",
    "features_train = pd.DataFrame(scaler.transform(features_train), index=features_train.index,columns=features_train.columns)\n",
    "# features_train_val = scaler.transform(features_train_val)\n",
    "features_val = pd.DataFrame(scaler.transform(features_val), index=features_val.index,columns=features_val.columns)\n",
    "features_test = pd.DataFrame(scaler.transform(features_test), index=features_test.index,columns=features_test.columns)\n",
    "\n",
    "del features_train_val\n",
    "del labels_train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando somente a confiança:     26 FP em 1152 exemplos (2.26%)\n",
      "Usando somente a confiança:      7 FP em  290 exemplos (2.41%)\n",
      "Usando somente a confiança:      9 FP em  353 exemplos (2.55%)\n",
      "\n",
      "AUC: 0.753 (sem 2ª rede)\n",
      "\n",
      "neighbors=11\n",
      "FP total no dl: 9 de 9\n",
      "\n",
      "[[8.         0.88888889]\n",
      " [8.         0.88888889]\n",
      " [8.         0.88888889]\n",
      " [8.         0.88888889]\n",
      " [9.         1.        ]\n",
      " [9.         1.        ]\n",
      " [9.         1.        ]\n",
      " [9.         1.        ]\n",
      " [9.         1.        ]\n",
      " [9.         1.        ]]\n",
      "\n",
      "AUC: 0.957 (atb = 90)\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = dataset+'saved_models/bin_'+f\n",
    "model = tf.keras.models.load_model(model_path+'.h5',compile=False)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "shap_type = 'nn'\n",
    "\n",
    "\n",
    "shap_path = 'C:/Users/SE8ALUNO/maj_ricardo/google/doutorado/duvidas/'+dataset+'shap/'+f+'/'+shap_type+'/'\n",
    "# shap_path = 'J:/Meu Drive/SE8ALUNO/dataset/'+dataset+'shap/'+f+'/'+shap_type+'/'\n",
    "\n",
    "features = {'train':features_train,'val':features_val, 'test':features_test}\n",
    "labels = {'train':labels_train, 'val':labels_val, 'test':labels_test}\n",
    "\n",
    "\n",
    "\n",
    "seed = 42\n",
    "\n",
    "\n",
    "for cj in ['train','val','test']:\n",
    "\n",
    "#     prediction = model.predict(features[cj]).squeeze() # com gpu\n",
    "    prediction = tf.squeeze(model(features[cj].values)).numpy()\n",
    "\n",
    "    msc_p = prediction > 0.5\n",
    "    msc_fp = msc_p & (labels[cj] == 0)\n",
    "    if cj == 'train':\n",
    "        if dataset+f == 'TrafficLabelling/CPdId':\n",
    "            conf_limiar = prediction[msc_fp].max()\n",
    "        else:\n",
    "            conf_limiar = np.percentile(prediction[msc_fp],99, interpolation='higher')\n",
    "        msc_p_train = msc_p & (prediction <= conf_limiar)\n",
    "        dl = {}\n",
    "\n",
    "    msc_p = msc_p & (prediction <= conf_limiar) # diferente do adversarial, pois aqui é só o que está abaixo do limiar\n",
    "    filename = shap_path+'shap_values_'+cj+'.npy'\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        shap_values = np.load(filename)\n",
    "    elif shap_type.startswith('kernel'):\n",
    "        link='identity'\n",
    "        if '_k' in shap_type:\n",
    "            background = shap.kmeans(features['train'][msc_p_train],50)\n",
    "        else:\n",
    "            background = shap.sample(features['train'][msc_p_train], 100,random_state=seed)\n",
    "        explainer = shap.KernelExplainer(model.predict, background, link=link)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            shap_values = explainer.shap_values(features[cj][msc_p])[0]\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        np.save(filename, shap_values)\n",
    "        mail_content = '{0}, {1}, {2} ok!!'.format(shap_type,f,cj)\n",
    "        mensagem(mail_content)\n",
    "    elif shap_type.startswith('nn'):\n",
    "        # explainer = shap.DeepExplainer(model.predict, features['train'][msc_p_train].values)\n",
    "        explainer = shap.DeepExplainer(model, shap.sample(features['train'][msc_p_train], 2500,random_state=seed).values)\n",
    "        shap_values = explainer.shap_values(features[cj][msc_p].values)[0]\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        np.save(filename, shap_values)\n",
    "#         mail_content = '{0}, {1}, {2} ok!! at se8'.format(shap_type,f,cj)\n",
    "#         mensagem(mail_content)\n",
    "\n",
    "    labels2 = labels[cj][msc_p,np.newaxis] == 0\n",
    "    cf_fp, cf_size = labels2.sum(), labels2.shape[0]\n",
    "    \n",
    "    print('Usando somente a confiança: {:>6d} FP em {:>4d} exemplos ({:2.2f}%)'.format(cf_fp, cf_size, 100*cf_fp/cf_size))\n",
    "    if modo == 'cos':\n",
    "        if cj == 'train':\n",
    "            VP = shap_values[labels[cj][msc_p] != 0].mean(axis=0)\n",
    "            FP = shap_values[labels[cj][msc_p] == 0].mean(axis=0)\n",
    "            vp_fp = np.array([VP, FP])\n",
    "\n",
    "            # Similaridade entre a média dos gradientes (VP e FP) no conjunto treino\n",
    "            print('Similaridade:',float(cosine_similarity(VP[np.newaxis,:], FP[np.newaxis,:])))\n",
    "            # Normas das médias dos gradientes (VP e FP) no conjunto treino\n",
    "            norma_vp_fp = np.linalg.norm(vp_fp, axis=1)\n",
    "            print('||VP|| = {}\\n||FP|| = {}'.format(norma_vp_fp[0],norma_vp_fp[1]))\n",
    "\n",
    "        dados = np.concatenate((prediction[msc_p,np.newaxis],cosine_similarity(shap_values, vp_fp), np.linalg.norm(shap_values,axis=1, keepdims=True),labels2), axis=1)\n",
    "        dados = pd.DataFrame(dados, index=features[cj][msc_p].index, columns=['Confiança','cos(\\u03B8)\\u2207_VP','cos(\\u03B8)\\u2207_FP','||\\u2207||','FP?'])\n",
    "\n",
    "        refin = 0\t\t\t\t\t\n",
    "        adv_fp, adv_size = (((dados['cos(\\u03B8)\\u2207_FP']-dados['cos(\\u03B8)\\u2207_VP'])>=refin) & dados['FP?']).sum(), ((dados['cos(\\u03B8)\\u2207_FP']-dados['cos(\\u03B8)\\u2207_VP'])>=refin).sum()\n",
    "        # cf_idx = dl[cj][dl[cj]['FP?']].iloc[-adv_fp].name\n",
    "        # cf_idx_size = cf_size - np.flatnonzero(dados.index == cf_idx).squeeze()\n",
    "        print('Usando confiança + SHAP: {:>9d} FP em {:>4d} exemplos ({:2.2f}%)'.format(adv_fp, adv_size, 100*adv_fp/adv_size))\n",
    "        # print('A partir do menos confiável: {:>5d} FP em {:>4d} exemplos ({:2.2f}%)\\n'.format(adv_fp, cf_idx_size, 100*adv_fp/cf_idx_size), file=sumario)\n",
    "        \n",
    "\n",
    "    elif modo == 'pca':\n",
    "        \n",
    "        shap_values = features[cj][msc_p].values\n",
    "        dados = np.concatenate((prediction[msc_p,np.newaxis],shap_values, labels2), axis=1)\n",
    "        dados = pd.DataFrame(dados, index=features[cj][msc_p].index, columns=['Confiança']+features[cj].columns.tolist()+['FP?'])\n",
    "\n",
    "    elif modo == 'pca_9':\n",
    "        if cj == 'train':\n",
    "            from sklearn.decomposition import PCA\n",
    "            n=9\n",
    "            pca = PCA(n_components=n, random_state=seed)\n",
    "            pca.fit(shap_values)\n",
    "\n",
    "        shap_values = pca.transform(shap_values)\n",
    "        dados = np.concatenate((prediction[msc_p,np.newaxis],shap_values, labels2), axis=1)\n",
    "        dados = pd.DataFrame(dados, index=features[cj][msc_p].index, columns=['Confiança']+['pca'+str(i) for i in range(n)]+['FP?'])\n",
    "\n",
    "    dados.sort_values('Confiança',ascending=False,inplace=True)\n",
    "    dados = dados.astype({'FP?': 'bool'})\n",
    "\n",
    "    dl[cj] = dados\n",
    "\n",
    "\n",
    "\n",
    "labels['train'] = dl['train']['FP?']\n",
    "features['train'] = dl['train'].iloc[:, :-1]\n",
    "# LE.fit(labels['train'])\n",
    "# labels['train'] = LE.transform(labels['train'])\n",
    "scaler.fit(features['train'])\n",
    "features['train'] = scaler.transform(features['train'])\n",
    "labels['val'] = dl['val']['FP?']\n",
    "features['val'] = dl['val'].iloc[:, 0:-1]\n",
    "# labels['val'] = LE.transform(labels['val'])\n",
    "features['val'] = scaler.transform(features['val'])\n",
    "\n",
    "labels['test'] = dl['test']['FP?']\n",
    "features['test'] = dl['test'].iloc[:, 0:-1]\n",
    "# labels['test'] = LE.transform(labels['test'])\n",
    "features['test'] = scaler.transform(features['test'])\n",
    "\n",
    "cj='test'\n",
    "if (dataset == 'TrafficLabelling/') & (f in ['C','CPd_dest']):\n",
    "    p = np.linspace(0,.25,11)\n",
    "else:\n",
    "    p = np.linspace(0,1,11)\n",
    "fracoes = dl[cj].shape[0]*p\n",
    "fracoes = fracoes.round().astype(int)[1:]\n",
    "# print(p)\n",
    "# print(fracoes)\n",
    "\n",
    "tot_fp = msc_fp.sum()\n",
    "dx = p[2]-p[1]\n",
    "cj_analise = np.zeros(fracoes.shape,dtype='int')\n",
    "for i,j in enumerate(fracoes):\n",
    "    cj_analise[i] = dl[cj].iloc[-j:,-1].sum()\n",
    "# print(cj_analise)\n",
    "\n",
    "\n",
    "print('\\nAUC: {:.3f} (sem 2ª rede)'.format(np.trapz(cj_analise/tot_fp, dx=dx)/((len(cj_analise)-1)*dx)))\n",
    "\n",
    "\n",
    "\n",
    "rodadas = 20\n",
    "\n",
    "if modo == 'pca_9':\n",
    "    atbs = np.floor(np.linspace(0,dados.shape[1]-1,4)[1:]).astype(int)\n",
    "else:\n",
    "    atbs = [dados.shape[1]-1]# colocar 1 antes\n",
    "\n",
    "# atb = dados.shape[1]-1 # 1 para apenas confinça, 4 para confiança + aprox adv\n",
    "# atb = 1 # 1 para apenas confinça, 4 para confiança + aprox adv\n",
    "for atb in atbs[-1:]:\n",
    "    cj_analise = np.zeros(fracoes.shape)\n",
    "\n",
    "    if segunda == 'mlp':\n",
    "        dl2 = np.zeros(labels[cj].shape)\n",
    "        for n in range(rodadas):\n",
    "            model2 = tf.keras.models.Sequential([tf.keras.layers.Dense(10, input_dim=features['train'][:,:atb].shape[1], activation='relu'), tf.keras.layers.Dense(10, activation='relu'), tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "            model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', verbose=1, patience=5, restore_best_weights=True)\n",
    "            model2.fit(features['train'][:,:atb], labels['train'], epochs=150, verbose=0, batch_size = 256, validation_data=(features['val'][:,:atb], labels['val']), callbacks=[es])\n",
    "\n",
    "            dl2 += model2.predict(features[cj][:,:atb]).squeeze()\n",
    "\n",
    "        dl2 = dl2/rodadas\n",
    "        msg = ''\n",
    "\n",
    "    elif segunda == 'knn':\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        fracoes_val = dl['val'].shape[0]*p\n",
    "        fracoes_val = fracoes_val.round().astype(int)[1:]\n",
    "        # cj_analise = np.zeros(fracoes.shape)\n",
    "        aucs = np.zeros(rodadas)\n",
    "        for l in range(1, rodadas):\n",
    "            model2 = KNeighborsClassifier(n_neighbors=l)\n",
    "            model2.fit(features['train'][:,1:atb], labels['train'])\n",
    "            dl2 = model2.predict_proba(features['val'][:,1:atb])\n",
    "            dl2 = pd.DataFrame(np.column_stack((dl2[:,1],labels['val'])),index=labels['val'].index,columns=['Conf2',labels['val'].name])\n",
    "            dl2.sort_values(by=['Conf2'], inplace=True)\n",
    "            for i,j in enumerate(fracoes_val):\n",
    "                cj_analise[i] = dl2.iloc[-j:,-1].sum()\n",
    "            aucs[l] = np.trapz(cj_analise/tot_fp, dx=dx)/((len(cj_analise)-1)*dx)\n",
    "\n",
    "        n_neighbors=aucs.argmax()\n",
    "        model2 = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "        model2.fit(features['train'][:,1:atb], labels['train'])\n",
    "        dl2 = model2.predict_proba(features['test'][:,1:atb])[:,1]\n",
    "        msg = '\\nneighbors={:d}'.format(n_neighbors)\n",
    "\n",
    "    dl2 = pd.DataFrame(np.column_stack((dl2,labels[cj])),index=labels[cj].index,columns=['Conf2',labels[cj].name])\n",
    "    dl2.sort_values(by=['Conf2'], inplace=True)\n",
    "    for i,j in enumerate(fracoes):\n",
    "        cj_analise[i] = dl2.iloc[-j:,-1].sum()\n",
    "    \n",
    "    print('{:s}\\nFP total no dl: {:d} de {:d}\\n'.format(msg,cf_fp,tot_fp))\n",
    "    print(np.column_stack((cj_analise, cj_analise/tot_fp)))\n",
    "    print('\\nAUC: {:.3f} (atb = {:d})'.format(np.trapz(cj_analise/tot_fp, dx=dx)/((len(cj_analise)-1)*dx), atb))\n",
    "    print('\\n------------------------------------------------------------\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "atb: 90\n",
      "FP total no dl: 9 de 9\n",
      "\n",
      "AUC: 0.883\n",
      "\n",
      "[[6.         0.66666667]\n",
      " [8.         0.88888889]\n",
      " [8.         0.88888889]\n",
      " [8.         0.88888889]\n",
      " [8.         0.88888889]\n",
      " [8.         0.88888889]\n",
      " [8.         0.88888889]\n",
      " [8.         0.88888889]\n",
      " [8.         0.88888889]\n",
      " [9.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# dl2 = dl2_final\n",
    "# dl2 = dl2_adv\n",
    "dl2.columns = ['Conf2','FP?']\n",
    "tot_confs = dl2['Conf2'].value_counts()\n",
    "tot_confs.sort_index(ascending=False,inplace=True)\n",
    "fp_confs = dl2[dl2['FP?'] == 1]['Conf2'].value_counts()\n",
    "tot_conf_counter = 1\n",
    "for i,j in enumerate(fracoes):\n",
    "  while j > tot_confs.iloc[:tot_conf_counter].sum():\n",
    "    tot_conf_counter += 1\n",
    "  fp_ant = fp_confs[fp_confs.index > tot_confs.index[tot_conf_counter-1]].sum()\n",
    "  residuo = j - tot_confs.iloc[:tot_conf_counter-1].sum()\n",
    "  if np.isin(tot_confs.index[tot_conf_counter-1],fp_confs.index,assume_unique=True):\n",
    "    dens = fp_confs[tot_confs.index[tot_conf_counter-1]]/tot_confs.iloc[tot_conf_counter-1]\n",
    "  else:\n",
    "    dens=0\n",
    "  # dens = fp_confs[tot_confs.index[tot_conf_counter-1]]/tot_confs.iloc[tot_conf_counter-1]\n",
    "  cj_analise[i] = fp_ant + residuo*dens\n",
    "cj_analise = np.floor(cj_analise)\n",
    "print('\\natb: {:d}\\nFP total no dl: {:d} de {:d}\\n'.format(atb,cf_fp,tot_fp))\n",
    "print('AUC: {:.3f}\\n'.format(np.trapz(cj_analise/tot_fp, dx=dx)/((len(cj_analise)-1)*dx)))\n",
    "print(np.column_stack((cj_analise, cj_analise/tot_fp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl2_path = 'C:/Users/SE8ALUNO/maj_ricardo/google/doutorado/duvidas/'+dataset+'shap/dl2_'+f+'.csv'\n",
    "\n",
    "# dl2.rename(columns={'Conf2':'Conf2_shap','FP?':'FP?_shap'}, inplace=True)\n",
    "# dl2.to_csv(dl2_path)\n",
    "dl2 = pd.read_csv(dl2_path,index_col=0,float_precision='round_trip')\n",
    "\n",
    "for i,j in enumerate(fracoes):\n",
    "  cj_analise[i] = dl2.iloc[-j:,-1].sum()\n",
    "print('AUC: {:.3f}\\n'.format(np.trapz(cj_analise/tot_fp, dx=dx)/((len(cj_analise)-1)*dx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl2.rename(columns={'Conf2':'Conf2_shap','FP?':'FP?_shap'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl2_path_adv = 'C:/Users/SE8ALUNO/maj_ricardo/google/doutorado/duvidas/'+dataset+'adv/dl2_'+f+'.csv'\n",
    "\n",
    "dl2_adv = pd.read_csv(dl2_path_adv,index_col=0,float_precision='round_trip')\n",
    "\n",
    "for i,j in enumerate(fracoes):\n",
    "  cj_analise[i] = dl2_adv.iloc[-j:,-1].sum()\n",
    "# print('\\natb: {:d}\\nFP total no dl: {:d} de {:d}\\n'.format(atb,cf_fp,tot_fp))\n",
    "print('AUC: {:.3f}\\n'.format(np.trapz(cj_analise/tot_fp, dx=dx)/((len(cj_analise)-1)*dx)))\n",
    "# print(np.column_stack((cj_analise, cj_analise/tot_fp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl2_final = pd.concat([dl2, dl2_adv], axis=1)\n",
    "assert dl2.shape[0] == dl2_adv.shape[0], 'dls devem ter o mesmo tamanho'\n",
    "assert dl2_final.shape[0] == dl2.shape[0], 'indexes devem ser os mesmos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl2_final = pd.concat([dl2, dl2_adv], axis=1)\n",
    "dl2_final = pd.concat([dl2_final[['Conf2_shap','Conf2_adv']].mean(axis=1),dl2_final['FP?_adv']], axis=1)\n",
    "dl2_final.columns = ['Conf2','FP?']\n",
    "dl2_final.sort_values(by=['Conf2'], inplace=True)\n",
    "for i,j in enumerate(fracoes):\n",
    "  cj_analise[i] = dl2_final.iloc[-j:,-1].sum()\n",
    "# print('\\natb: {:d}\\nFP total no dl: {:d} de {:d}\\n'.format(atb,cf_fp,tot_fp))\n",
    "print('AUC: {:.3f}\\n'.format(np.trapz(cj_analise/tot_fp, dx=dx)/((len(cj_analise)-1)*dx)))\n",
    "print(np.column_stack((cj_analise, cj_analise/tot_fp)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
